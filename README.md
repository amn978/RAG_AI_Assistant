ğŸ§  RAG AI Assistant (Recruiter-Based Knowledge System)

A Retrieval-Augmented Generation (RAG) powered AI assistant that provides accurate, context-aware answers from custom documents using vector search and local LLM inference.

Built for interview preparation, recruiter systems, technical knowledge bases, and enterprise documentation Q&A.

ğŸš€ Features:

ğŸ” Semantic Search using FAISS vector database

ğŸ§  RAG Pipeline (Retrieval + LLM Generation)

ğŸ¤– Local LLM Inference using Ollama (tinyllama)

ğŸ“„ PDF / Document Ingestion

ğŸ§¬ HuggingFace Embeddings (all-MiniLM-L6-v2)

âš¡ FastAPI Backend

ğŸ§± Modular architecture

ğŸ“š Source-based answers with references

ğŸ” No hallucination policy (context-only answering)

ğŸ  Fully local system (no cloud dependency)

ğŸ“¡ API-based architecture (production-ready)

ğŸ¯ Problem Statement:

Recruiters, students, and professionals face problems like:

Scattered learning resources

Time wasted searching PDFs and notes

Inaccurate AI answers (hallucinations)

No context-based responses

Lack of personalized knowledge systems

ğŸ’¡ Reason Behind Building This System

To create:

A trusted AI assistant

That answers only from verified documents

Provides source-backed answers

Works offline

Is recruiter-friendly

Is interview-prep focused

Is enterprise scalable

ğŸ§© Solution:

This project uses:

RAG (Retrieval-Augmented Generation)

Meaning:

User asks a question

System searches vector database

Retrieves relevant document chunks

Builds context

LLM answers only from that context

Returns answer + sources

âœ… No hallucination
âœ… No fake answers
âœ… No guessing
âœ… No external knowledge injection

ğŸ§± System Architecture:
User Query
   â†“
FastAPI API
   â†“
FAISS Vector Search
   â†“
Relevant Docs Retrieved
   â†“
Context Builder
   â†“
RAG Prompt
   â†“
Local LLM (TinyLlama via Ollama)
   â†“
Answer + Sources

ğŸ› ï¸ Tech Stack:

Python

FastAPI

LangChain

FAISS

Ollama

TinyLlama

HuggingFace Transformers

SentenceTransformers

Vector Embeddings

ğŸŒ Real-World Applications:

ğŸ“ Interview Preparation System

ğŸ§‘â€ğŸ’¼ Recruiter Knowledge Assistant

ğŸ¢ Enterprise Knowledge Base

ğŸ“š Document Q&A System

ğŸ§  AI Study Assistant

ğŸ§¾ Compliance Document Search

ğŸ“‘ Legal/Medical Document Search

ğŸ« Academic Assistant

ğŸ“Š Research Assistant

ğŸ§‘â€ğŸ« Training System

ğŸ§ª API Example:

POST /ask
{
  "question": "What is Python GIL?"
}
Response:
{
  "answer": "...",
  "sources": [
    {
      "source": "data/docs/python_interview_notes.pdf",
      "page": 6
    }
  ]
}

ğŸ“ Project Structure:

Rag_AI_Assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ prompts.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ docs/
â”‚
â”œâ”€â”€ models/          # ignored in git
â”œâ”€â”€ vector_store/    # ignored in git
â”‚
â”œâ”€â”€ test_rag.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore

âš™ï¸ Setup Instructions:

pip install -r requirements.txt
ollama pull tinyllama
uvicorn app.main:app --reload

PDF Parsing

RAG Architecture
